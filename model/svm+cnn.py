# -*- coding: utf-8 -*-
"""SVM+CNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNtIVihKVE3d0K1e0IpHR-Nl0WHNGFPU
"""

import os
import numpy as np
from tqdm import tqdm
import joblib
import matplotlib.pyplot as plt
import pandas as pd
import shutil

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (confusion_matrix, classification_report,
                             roc_auc_score, roc_curve, precision_recall_curve, auc)

from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input

import kagglehub

# Download latest version
path = kagglehub.dataset_download("divg07/casia-20-image-tampering-detection-dataset")

print("Path to dataset files:", path)

class Config:
    DATA_DIR = "/kaggle/input/casia-20-image-tampering-detection-dataset/CASIA2"
    real_dir = os.path.join(DATA_DIR, "Au")
    fake_dir = os.path.join(DATA_DIR, "Tp")

out = '/kaggle/working/dataset_amplified/'
OUT_REAL = os.path.join(out, 'real')
OUT_FAKE= os.path.join(out, 'fake')
os.makedirs(OUT_REAL, exist_ok=True)
os.makedirs(OUT_FAKE, exist_ok=True)

#Preprocess

real_src = Config.real_dir
fake_src = Config.fake_dir

print("Copying REAL images...")
for fname in tqdm(os.listdir(real_src)):
    if fname.lower().endswith((".jpg", ".jpeg", ".png")):
        shutil.copy(os.path.join(real_src, fname), os.path.join(OUT_REAL, fname))

print("Copying FAKE images...")
for fname in tqdm(os.listdir(fake_src)):
    if fname.lower().endswith((".jpg", ".jpeg", ".png")):
        shutil.copy(os.path.join(fake_src, fname), os.path.join(OUT_FAKE, fname))

FEATURE_FILE = 'features_casia2.npz'
IMG_SIZE = (224, 224)
X = None
y = None
if os.path.exists(FEATURE_FILE):
    print("Loading saved features from", FEATURE_FILE)
    data = np.load(FEATURE_FILE)
    X = data['X']
    y = data['y']

# If not loaded, extract features from images
if X is None:
    print("No saved features found â€” extracting features (this may take time)...")
    if not (os.path.isdir(OUT_REAL) and os.path.isdir(OUT_FAKE)):
        raise FileNotFoundError(f"Could not find dataset folders:\n {OUT_REAL}\n {OUT_FAKE}\n"
                                "Adjust the OUT_REAL / OUT_FAKE paths before running.")
    real_paths = [os.path.join(OUT_REAL, f) for f in os.listdir(OUT_REAL) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    fake_paths = [os.path.join(OUT_FAKE, f) for f in os.listdir(OUT_FAKE) if f.lower().endswith(('.jpg','.jpeg','.png'))]
    print("Found:", len(real_paths), "real images,", len(fake_paths), "fake images")

    feature_extractor = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=(224,224,3))

    def extract_features(img_path):
        img = image.load_img(img_path, target_size=IMG_SIZE)
        arr = image.img_to_array(img)
        arr = np.expand_dims(arr, 0)
        arr = preprocess_input(arr)
        feat = feature_extractor.predict(arr, verbose=0)
        return feat.flatten()

    X_list = []
    y_list = []

    print("Extracting REAL features...")
    for p in tqdm(real_paths):
        X_list.append(extract_features(p))
        y_list.append(1)

    print("Extracting FAKE features...")
    for p in tqdm(fake_paths):
        X_list.append(extract_features(p))
        y_list.append(0)

    X = np.vstack(X_list)
    y = np.array(y_list)

    # Save for future fast runs
    np.savez_compressed(FEATURE_FILE, X=X, y=y)
    print("Saved features to:", FEATURE_FILE)

# Confirm shapes
print("Feature matrix shape:", X.shape)
print("Labels shape:", y.shape)

# Fast 80/20 stratified split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)

# Scale
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

# Train balanced SVM
svm = SVC(kernel='rbf', probability=True, class_weight={0:3, 1:1})
svm.fit(X_train_s, y_train)

# Predict & evaluate
y_proba = svm.predict_proba(X_test_s)[:,1]
y_pred = (y_proba >= 0.4).astype(int)

print("AUC:", roc_auc_score(y_test, y_proba))
print("\nClassification report:\n")
print(classification_report(y_test, y_pred, target_names=['FAKE(0)', 'REAL(1)']))

# Save model & scaler (optional)
joblib.dump(svm, 'svm_casia2_fast.joblib')
joblib.dump(scaler, 'scaler_casia2_fast.joblib')
print("Saved svm_casia2_fast.joblib and scaler_casia2_fast.joblib")

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report


cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print("Confusion matrix (raw):")
print(cm)
print(f"TN={tn}, FP={fp}, FN={fn}, TP={tp}\n")

# Optional: classification report
print("Classification report:\n")
print(classification_report(y_test, y_pred, target_names=['FAKE(0)', 'REAL(1)']))

# Save confusion matrix counts to CSV
cm_df = pd.DataFrame(cm, index=['True_FAKE','True_REAL'], columns=['Pred_FAKE','Pred_REAL'])
cm_df.to_csv('confusion_matrix_counts.csv', index=True)
print("\nSaved confusion_matrix_counts.csv")

# 1) Recreate the feature extractor (weights cached if already downloaded)
feature_extractor = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=(224,224,3))

# 2) Define the feature extraction function
IMG_SIZE = (224, 224)
def extract_features(img_path):
    img = image.load_img(img_path, target_size=IMG_SIZE)
    arr = image.img_to_array(img)
    arr = np.expand_dims(arr, 0)
    arr = preprocess_input(arr)
    feat = feature_extractor.predict(arr, verbose=0)
    return feat.flatten()

# 3) Load model & scaler if needed
import joblib
svm_final = joblib.load('svm_casia2_final.joblib')
scaler = joblib.load('scaler_casia2_final.joblib')

# 4) Predict a new image
new_img_path = '/content/drive/MyDrive/0001-4159561663888291018.png'
feat = extract_features(new_img_path)
feat_scaled = scaler.transform([feat])

pred_label = svm_final.predict(feat_scaled)[0]
pred_prob = svm_final.predict_proba(feat_scaled)[0,1]    # probability for REAL class
pred_prob_fake = 1 - pred_prob
print("Confidence for FAKE:", round(pred_prob_fake,4))

print("Prediction:", "REAL" if pred_label==1 else "FAKE")
print("Confidence (REAL prob):", round(pred_prob, 4))

new_img_path = '/content/drive/MyDrive/rimg.png'
feat = extract_features(new_img_path)
feat_scaled = scaler.transform([feat])

pred_label = svm_final.predict(feat_scaled)[0]
pred_prob = svm_final.predict_proba(feat_scaled)[0,1]    # probability for REAL class
pred_prob_fake = 1 - pred_prob
print("Confidence for FAKE:", round(pred_prob_fake,4))

print("Prediction:", "REAL" if pred_label==1 else "FAKE")
print("Confidence (REAL prob):", round(pred_prob, 4))